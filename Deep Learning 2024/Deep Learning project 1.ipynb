{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ea337813-fce5-4417-b59b-dc0d15fab55e",
      "metadata": {
        "id": "ea337813-fce5-4417-b59b-dc0d15fab55e"
      },
      "source": [
        "# Deep Learning - Report 1\n",
        "This is a template notebook of your report. Please complete your report with your team mate following these instructions:\n",
        "- Work on the exercies below by filling the notebook.\n",
        "- **Rename your notebook** in the format `FirstName1FAMILYNAME1_FirstName2FAMILYNAME2_report1.ipynb`. For example, when the team consists of Johann FAOUZI and Ikko Yamane, the file name should look like `JohannFAOUZI_IkkoYAMANE_report1.ipynb`.\n",
        "- You are only allowed to edit new cells you have added (except the \"Solution to Exercise 0\" cell).\n",
        "- Write `### Solution to Exercise (number)` at the beginning of each cell you add.\n",
        "- Please submit your notebook on Moodle.\n",
        "- The submission deadline is 17:00 (UTC+2) of September 19, 2023.\n",
        "- Explain your code with comment or/and markdown. The explanations will be taken into account for the evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "794f604e-1919-45c0-8fed-31d0c128327c",
      "metadata": {
        "id": "794f604e-1919-45c0-8fed-31d0c128327c"
      },
      "source": [
        "## Exercise 0\n",
        "- Rename your notebook in the format `FirstName1FAMILYNAME1_FirstName2FAMILYNAME2_report1.ipynb`.\n",
        "- Write your names and email addresses."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46bbec6f-db8a-45d6-972a-282f11e2c383",
      "metadata": {
        "id": "46bbec6f-db8a-45d6-972a-282f11e2c383"
      },
      "source": [
        "### Solution to Exercise 0\n",
        "- Name of Author 1: GANIYU Isaac\n",
        "- Name of Author 2: DECROS Florian\n",
        "- Email address of Author 1: isaac.ganiyu@eleve.ensai.fr\n",
        "- Email address of Author 2: florian.decros@eleve.ensai.fr"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ac3d7fe-0654-415f-8fb3-cfb1007bda14",
      "metadata": {
        "id": "0ac3d7fe-0654-415f-8fb3-cfb1007bda14"
      },
      "source": [
        "## CIFAR-10\n",
        "We are going to work on the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) [Krizhevsky 2009].\n",
        "\n",
        "For the purpose of testing your skills, we are going to directly download an original dataset and manually adapt it to the PyTorch format. The following three cells download the data, create NumPy arrays of them, and show examples. The `load_cifar10` function converts the color images to gray-scale ones when `color=False`.\n",
        "\n",
        "[Krizhevsky 2009] [Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, 2009.](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dc9eab2-e508-4bb2-b50b-13c83f57531b",
      "metadata": {
        "id": "3dc9eab2-e508-4bb2-b50b-13c83f57531b"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import os.path\n",
        "from pathlib import Path\n",
        "\n",
        "def download_cifar10():\n",
        "    filename = 'cifar-10.tar.gz'\n",
        "    if os.path.isfile(filename):\n",
        "        print(f'{filename} already exists. Skipping downloading.')\n",
        "        return\n",
        "\n",
        "    url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
        "\n",
        "    with urllib.request.urlopen(url) as testfile, open('cifar-10.tar.gz', 'wb') as f:\n",
        "        f.write(testfile.read())\n",
        "\n",
        "\n",
        "def extract_cifar10(filename=\"cifar-10.tar.gz\"):\n",
        "    dirname = 'cifar-10-batches-py'\n",
        "    if Path(dirname).is_dir():\n",
        "        print(f'{dirname} already exists. Skipping extracting.')\n",
        "        return\n",
        "\n",
        "    tar = tarfile.open(filename)\n",
        "    tar.extractall()\n",
        "    tar.close()\n",
        "\n",
        "\n",
        "def load_cifar10(train, dir='cifar-10-batches-py', color=False):\n",
        "    data_raw = []\n",
        "    if train:\n",
        "        for i in range(5):\n",
        "            with open(f'{dir}/data_batch_{i+1}', 'rb') as f:\n",
        "                data_raw.append(pickle.load(f, encoding='bytes'))\n",
        "        x = np.concatenate(\n",
        "            [d[b'data'] for d in data_raw],\n",
        "            axis=0)\n",
        "        y = np.concatenate(\n",
        "            [d[b'labels'] for d in data_raw],\n",
        "            axis=0)\n",
        "    else:\n",
        "        with open(f'{dir}/test_batch', 'rb') as f:\n",
        "            data_raw = pickle.load(f, encoding='bytes')\n",
        "        x = np.array(data_raw[b'data'])\n",
        "        y = np.array(data_raw[b'labels'])\n",
        "\n",
        "    x = np.reshape(x, newshape=(len(x), 3, 32, 32))\n",
        "    if not color:\n",
        "        x = x.mean(axis=1, keepdims=True)  # Convert Red-Green-Blue (RGB) images to gray-scale.\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a43ec81c-28c3-4349-9ebb-3b9252c6146e",
      "metadata": {
        "id": "a43ec81c-28c3-4349-9ebb-3b9252c6146e"
      },
      "outputs": [],
      "source": [
        "download_cifar10()\n",
        "extract_cifar10()\n",
        "x_train_val_np, y_train_val_np = load_cifar10(train=True)\n",
        "x_test_np, y_test_np = load_cifar10(train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "066184ca-ee34-49ce-b848-e5dd4e02c3f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "066184ca-ee34-49ce-b848-e5dd4e02c3f6",
        "outputId": "50a10d09-fefa-469f-9373-2c11932d0a80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['truck', 'truck', 'deer', 'automobile', 'automobile']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x200 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "id2class = {\n",
        "    0: 'airplane',\n",
        "    1: 'automobile',\n",
        "    2: 'bird',\n",
        "    3: 'cat',\n",
        "    4: 'deer',\n",
        "    5: 'dog',\n",
        "    6: 'frog',\n",
        "    7: 'horse',\n",
        "    8: 'ship',\n",
        "    9: 'truck'\n",
        "}\n",
        "\n",
        "def plot_images(x, y, rows=1, cols=5, color=False):\n",
        "    figure = plt.figure(figsize=(2 * cols, 2 * rows))\n",
        "    ys = []\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            k = (i * cols) + j + 1\n",
        "            figure.add_subplot(rows, cols, k)\n",
        "            plt.axis(\"off\")\n",
        "            if color:\n",
        "                plt.imshow(np.transpose(x[k], [1, 2, 0]))\n",
        "            else:\n",
        "                plt.imshow(np.transpose(x[k], [1, 2, 0]), cmap=\"gray\")\n",
        "            ys.append(y[k])\n",
        "    print([id2class[id] for id in ys])\n",
        "    plt.show()\n",
        "\n",
        "plot_images(x_train_val_np, y_train_val_np, rows=1, cols=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ee7685e-d3ba-45e5-80a0-d48c2b61e2b7",
      "metadata": {
        "id": "5ee7685e-d3ba-45e5-80a0-d48c2b61e2b7"
      },
      "source": [
        "## Exercise 1\n",
        "Randomly split the dataset `(x_train_val_np, y_train_val_np)` to a training set `(x_train_np, y_train_np)` and a validation set `(x_val_np, y_val_np)`. Here, take `40000` data points for the training set and put the rest in the validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution to Exercise 1"
      ],
      "metadata": {
        "id": "cDWZ5jlJlFHk"
      },
      "id": "cDWZ5jlJlFHk"
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train_val_np.shape)\n",
        "y_train_val_np.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fsFPQUij65r",
        "outputId": "817d0cf0-b371-4ad9-88a3-fd0f5f947ccc"
      },
      "id": "4fsFPQUij65r",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 1, 32, 32)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000,)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "708d2a8c-b913-44f6-a0fa-f24d893a13a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "708d2a8c-b913-44f6-a0fa-f24d893a13a6",
        "outputId": "e662d84e-f0a0-433b-e701-a92cae2a4c81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shapes of x_train_np:  (40000, 1, 32, 32)\n",
            "shapes of x_val_np:  (10000, 1, 32, 32)\n",
            "shapes of y_train_np:  (40000,)\n",
            "shapes of y_val_np:  (10000,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train_np, x_val_np, y_train_np, y_val_np = train_test_split(x_train_val_np, y_train_val_np, train_size=40000,\n",
        "                                                              stratify = y_train_val_np, random_state = 6)\n",
        "# we use the stratify parameter to have the same class distribution in the sets and the random_state parameter to fix the randomness\n",
        "print(\"shapes of x_train_np: \", x_train_np.shape)\n",
        "print(\"shapes of x_val_np: \", x_val_np.shape)\n",
        "print(\"shapes of y_train_np: \", y_train_np.shape)\n",
        "print(\"shapes of y_val_np: \", y_val_np.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e282dd87-695e-4cbc-b5de-bd9fdbc14bf2",
      "metadata": {
        "id": "e282dd87-695e-4cbc-b5de-bd9fdbc14bf2"
      },
      "source": [
        "## Exercise 2\n",
        "Write code to convert `x_train_np, y_train_np, x_val_np, y_val_np, x_test_np, y_test_np` to PyTorch Tensors.\n",
        "Name the tensors as `x_train, y_train, x_val, y_val, x_test, y_test`, respectively.\n",
        "\n",
        "**Note**: You may need to explicitly change the `dtype` of your tensors. PyTorch by default requires the type (`dtype`) of input tensor to be `torch.float32` and that of the labels to be `torch.int64`."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution to Exercise 2"
      ],
      "metadata": {
        "id": "1Mhtr5fpl_f3"
      },
      "id": "1Mhtr5fpl_f3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f658ef6f-01dc-4f97-9149-498d34a33c2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f658ef6f-01dc-4f97-9149-498d34a33c2d",
        "outputId": "902f7ab9-fb80-478c-b810-8dac8bb57be9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x_train = torch.from_numpy(x_train_np) # convert from numpy to tensor\n",
        "x_train.to(dtype=torch.float32) #changing the dtype\n",
        "x_train.dtype"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = torch.from_numpy(y_train_np)\n",
        "y_train.to(dtype=torch.int64)\n",
        "y_train.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMh2Tjq2mtBD",
        "outputId": "72f184c7-7483-4fa9-9219-e70877793762"
      },
      "id": "EMh2Tjq2mtBD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = torch.from_numpy(x_val_np)\n",
        "x_val.to(dtype=torch.float32)\n",
        "x_val.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF13lfyNmuSg",
        "outputId": "19784239-c55e-46c3-faf3-d228474d424d"
      },
      "id": "uF13lfyNmuSg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_val = torch.from_numpy(y_val_np)\n",
        "y_val.to(dtype=torch.int64)\n",
        "y_val.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "718mQvn4muE-",
        "outputId": "c5f5ade2-b40d-4499-f1d3-1f8a705a37d4"
      },
      "id": "718mQvn4muE-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = torch.from_numpy(x_test_np)\n",
        "x_test.to(dtype=torch.float32)\n",
        "x_test.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyep-9dUnJIM",
        "outputId": "8250ec9a-c897-4d32-98a4-6b469cc0ebc6"
      },
      "id": "uyep-9dUnJIM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = torch.from_numpy(y_test_np)\n",
        "y_test.to(dtype=torch.int64)\n",
        "y_test.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0C_RN1LnI6r",
        "outputId": "f5d470f4-58dc-4752-c34e-461e79a0955e"
      },
      "id": "s0C_RN1LnI6r",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e29f160-2b66-4c6e-8443-b1c13067d6d9",
      "metadata": {
        "id": "0e29f160-2b66-4c6e-8443-b1c13067d6d9"
      },
      "source": [
        "## Exercise 3\n",
        "Write a Python class `CustomDataset` deriving `torch.utils.data.Dataset` and create dataloaders for the training, validation, and test sets.\n",
        "\n",
        "**Bonus** If possible, making the `transform` parameter and pass `ToTensor()` to it to avoid manually converting data to PyTorch tensors as in Exercise 2."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution of excercise 3"
      ],
      "metadata": {
        "id": "GkmcPb6MruGi"
      },
      "id": "GkmcPb6MruGi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "646ca6de-80eb-4090-847b-7476ca5cf369",
      "metadata": {
        "id": "646ca6de-80eb-4090-847b-7476ca5cf369"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor, Compose\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, X, y, transform=None):  # adding the transform parameter\n",
        "        self.X = torch.tensor(X, dtype=torch.float32) # applying the transform ToTensor() and changing the dtype directly\n",
        "        self.y = torch.tensor(y, dtype=torch.int64) # applying the transform ToTensor() and changing the dtype directly\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X_idx = self.X[idx]\n",
        "        y_idx = self.y[idx]\n",
        "\n",
        "        if self.transform: #we apply the transform before returning the data\n",
        "            X_idx = self.transform(X_idx)\n",
        "\n",
        "        return X_idx, y_idx\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms.v2 import ToDtype\n",
        "\n",
        "# creating the different dataloaders\n",
        "train_dataloader = DataLoader(CustomDataset(x_train, y_train), batch_size=64, shuffle=True) # shuffle parameter allows us to shuffle the data at each epoch\n",
        "val_dataloader = DataLoader(CustomDataset(x_val, y_val), batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(CustomDataset(x_test, y_test), batch_size=64, shuffle=True)\n"
      ],
      "metadata": {
        "id": "ukEiXbQzrnl2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be303ec8-f030-4822-a61a-b58edd03cbf2"
      },
      "id": "ukEiXbQzrnl2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
            "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
            "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
            "<ipython-input-12-d19eb33125b3>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.X = torch.tensor(X, dtype=torch.float32) # applying the transform ToTensor() and changing the dtype directly\n",
            "<ipython-input-12-d19eb33125b3>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.y = torch.tensor(y, dtype=torch.int64) # applying the transform ToTensor() and changing the dtype directly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87425561-86ce-456e-a747-e508a0cb22b3",
      "metadata": {
        "id": "87425561-86ce-456e-a747-e508a0cb22b3"
      },
      "source": [
        "## Exercise 4\n",
        "Let the variable `device` be `'cuda'` if CUDA (GPU) is available. Otherwise, let it be `'cpu'`.\n",
        "(Do **not** move the tensors from Exercise 1 to this `device` yet.)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution of excercise 4\n"
      ],
      "metadata": {
        "id": "0Zb1kCURzms0"
      },
      "id": "0Zb1kCURzms0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fbca087-4535-4152-ae4d-3d394eed3c0f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3fbca087-4535-4152-ae4d-3d394eed3c0f",
        "outputId": "7e80b3c3-0c13-4748-ebb9-2176e35434ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = \"cuda\"\n",
        "else :\n",
        "  device = \"cpu\"\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99fa22fc-91f0-4616-950f-f1cabdaaf170",
      "metadata": {
        "id": "99fa22fc-91f0-4616-950f-f1cabdaaf170"
      },
      "source": [
        "## Exercise 5\n",
        "Write a Python class `MLP5` for Multi-Layer Perceptron (MLP) with 5 layers derivng from `nn.Module` or `nn.Sequencial`.\n",
        "Your network must have the following sequential architecture:\n",
        "- First hidden layer: Linear layer (64 output features) + ReLU activation function\n",
        "- Second hidden layer: Linear layer (64 output features) + ReLU activation function\n",
        "- Third hidden layer: Linear layer (64 output features) + ReLU activation function\n",
        "- Forth hidden layer: Linear layer (64 output features) + ReLU activation function\n",
        "- Final layer: Linear layer\n",
        "\n",
        "Note that the final layer should have the output dimensionality equal to the number of classes in order to express class posterior probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution of excercise 5"
      ],
      "metadata": {
        "id": "lLkQj3Et1EUk"
      },
      "id": "lLkQj3Et1EUk"
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1G7Zgtk2dmg",
        "outputId": "38092f14-c7d7-41e3-f1c8-cbd4b07b07db"
      },
      "id": "S1G7Zgtk2dmg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "output_len = len(np.unique(y_train)) # number of classes\n",
        "\n",
        "# The class should inherit from the nn.Module and implement the two methods __init__ and forward\n",
        "class MLP5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(  # implementing the different layers\n",
        "            nn.Linear(1024, 64),  # 1024 is the input size\n",
        "            nn.ReLU(), # activation function\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, output_len),\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # The forward method defines the forward pass through the network.\n",
        "        x = self.flatten(x)\n",
        "        return self.linear_relu_stack(x)"
      ],
      "metadata": {
        "id": "TuBwBATL1D5B"
      },
      "id": "TuBwBATL1D5B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "77bddaf1-3077-4f55-9d52-a019520fd30c",
      "metadata": {
        "id": "77bddaf1-3077-4f55-9d52-a019520fd30c"
      },
      "source": [
        "## Exercise 6\n",
        "- Write a Python function for training a model with mini-batch updates for one epoch. Do not forget to move your mini-batch data to `device`.\n",
        "- Also, write Python function for evaluating the loss and the accuracy of a given model with a given dataloader."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution of exercise 6"
      ],
      "metadata": {
        "id": "6cQY2EfHQDgI"
      },
      "id": "6cQY2EfHQDgI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09f8098b-f1a6-40a5-a00a-31243356050e",
      "metadata": {
        "scrolled": true,
        "id": "09f8098b-f1a6-40a5-a00a-31243356050e"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, device):\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Display loss from time to time\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss: >7f}  [{current: >5d} / {size: >5d}]\")\n",
        "\n",
        "\n",
        "def evaluate_loop(dataloader, model, loss_fn, device, set_='Validation'):\n",
        "\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"\\n{set_} set: \\n  Accuracy: {(100 * correct): >0.1f}%, Avg loss: {test_loss: >8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14f0f65b-699f-40e4-9db0-e53668b549a8",
      "metadata": {
        "id": "14f0f65b-699f-40e4-9db0-e53668b549a8"
      },
      "source": [
        "## Exercise 7\n",
        "- Create an object using your class and keep it in the `modelMLP5` variable. Do not forget move your model to `device`.\n",
        "- Choose any loss function.\n",
        "- Create an optimizer for optimizing `modelMLP5`.\n",
        "- Train `modelMLP5` with your function(s) for 10 epochs. During the training, print the training and validation loss/accuracy every epoch.\n",
        "\n",
        "You may need to tune hyper-parameters such as the learning rate later while observing the behavior of the model during the training."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution of exercise 7"
      ],
      "metadata": {
        "id": "SRWdjiGAVbJe"
      },
      "id": "SRWdjiGAVbJe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bc1d5b4-68e0-458d-a936-8ba6b26da6ee",
      "metadata": {
        "scrolled": true,
        "id": "0bc1d5b4-68e0-458d-a936-8ba6b26da6ee"
      },
      "outputs": [],
      "source": [
        "modelMLP5 = MLP5().to(device) #object of class MLP5 and moving to device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Choosing cross entropy loss function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "d70WFDk0VkZ-"
      },
      "id": "d70WFDk0VkZ-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using stochastic gradient descent for the optimizer and 1e-3 for step-size\n",
        "learning_rate = 1e-3\n",
        "optimizer = torch.optim.SGD(modelMLP5.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "2GTfzPKOVuSj"
      },
      "id": "2GTfzPKOVuSj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, modelMLP5, loss_fn, optimizer, device)\n",
        "    evaluate_loop(val_dataloader, modelMLP5, loss_fn, device)"
      ],
      "metadata": {
        "id": "hAZcVTCLWF71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96016ca8-9a95-4b20-b394-1d6e423469dd"
      },
      "id": "hAZcVTCLWF71",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 3.496277  [   64 / 40000]\n",
            "loss: 2.239365  [ 6464 / 40000]\n",
            "loss: 2.126370  [12864 / 40000]\n",
            "loss: 2.300689  [19264 / 40000]\n",
            "loss: 1.992010  [25664 / 40000]\n",
            "loss: 2.082762  [32064 / 40000]\n",
            "loss: 2.095412  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 24.6%, Avg loss: 2.066419 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.054154  [   64 / 40000]\n",
            "loss: 1.967783  [ 6464 / 40000]\n",
            "loss: 1.966103  [12864 / 40000]\n",
            "loss: 1.954384  [19264 / 40000]\n",
            "loss: 2.030212  [25664 / 40000]\n",
            "loss: 2.033156  [32064 / 40000]\n",
            "loss: 2.104990  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 28.2%, Avg loss: 2.005207 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.927012  [   64 / 40000]\n",
            "loss: 1.979762  [ 6464 / 40000]\n",
            "loss: 2.017225  [12864 / 40000]\n",
            "loss: 1.821216  [19264 / 40000]\n",
            "loss: 1.921048  [25664 / 40000]\n",
            "loss: 2.097141  [32064 / 40000]\n",
            "loss: 2.037457  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 29.3%, Avg loss: 1.962265 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.841264  [   64 / 40000]\n",
            "loss: 1.951002  [ 6464 / 40000]\n",
            "loss: 1.974593  [12864 / 40000]\n",
            "loss: 1.772047  [19264 / 40000]\n",
            "loss: 2.070736  [25664 / 40000]\n",
            "loss: 1.994186  [32064 / 40000]\n",
            "loss: 2.000139  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 30.0%, Avg loss: 1.923354 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.110406  [   64 / 40000]\n",
            "loss: 1.920339  [ 6464 / 40000]\n",
            "loss: 1.815036  [12864 / 40000]\n",
            "loss: 1.892437  [19264 / 40000]\n",
            "loss: 1.894938  [25664 / 40000]\n",
            "loss: 1.981585  [32064 / 40000]\n",
            "loss: 1.979869  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 30.5%, Avg loss: 1.921518 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.977448  [   64 / 40000]\n",
            "loss: 1.900072  [ 6464 / 40000]\n",
            "loss: 1.831563  [12864 / 40000]\n",
            "loss: 1.962018  [19264 / 40000]\n",
            "loss: 1.802149  [25664 / 40000]\n",
            "loss: 1.867043  [32064 / 40000]\n",
            "loss: 1.922254  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 31.9%, Avg loss: 1.895859 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.718366  [   64 / 40000]\n",
            "loss: 1.934959  [ 6464 / 40000]\n",
            "loss: 1.885237  [12864 / 40000]\n",
            "loss: 1.712368  [19264 / 40000]\n",
            "loss: 1.658339  [25664 / 40000]\n",
            "loss: 1.954611  [32064 / 40000]\n",
            "loss: 1.754900  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 32.1%, Avg loss: 1.877128 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.584201  [   64 / 40000]\n",
            "loss: 1.999232  [ 6464 / 40000]\n",
            "loss: 1.846386  [12864 / 40000]\n",
            "loss: 1.838254  [19264 / 40000]\n",
            "loss: 2.016572  [25664 / 40000]\n",
            "loss: 1.907165  [32064 / 40000]\n",
            "loss: 1.874617  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 33.0%, Avg loss: 1.870608 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.939777  [   64 / 40000]\n",
            "loss: 1.852358  [ 6464 / 40000]\n",
            "loss: 1.902491  [12864 / 40000]\n",
            "loss: 1.825947  [19264 / 40000]\n",
            "loss: 1.786980  [25664 / 40000]\n",
            "loss: 1.897855  [32064 / 40000]\n",
            "loss: 1.995438  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 33.1%, Avg loss: 1.860364 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 2.066142  [   64 / 40000]\n",
            "loss: 1.851665  [ 6464 / 40000]\n",
            "loss: 2.001214  [12864 / 40000]\n",
            "loss: 1.838335  [19264 / 40000]\n",
            "loss: 1.937842  [25664 / 40000]\n",
            "loss: 1.731338  [32064 / 40000]\n",
            "loss: 1.718061  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 33.8%, Avg loss: 1.843725 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01758752-10ad-41ff-a9ae-3670408e291c",
      "metadata": {
        "id": "01758752-10ad-41ff-a9ae-3670408e291c"
      },
      "source": [
        "## Exercise 8\n",
        "Write a Python class `MLP5BN` similarly to `MLP5`, but put a batch normalization layer (`torch.nn.BatchNorm1d`) before every activation layer.\n",
        "Then, create an object using your class and keep it in the `modelMLP5BN` variable. Train this model and compare the results for `modelMLP5` and `modelMLP5BN`."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution of exercise 8"
      ],
      "metadata": {
        "id": "Dg7OqX3IspoH"
      },
      "id": "Dg7OqX3IspoH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69335fbf-f63f-478f-a39d-9ead47588ec1",
      "metadata": {
        "scrolled": true,
        "id": "69335fbf-f63f-478f-a39d-9ead47588ec1"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "# The class should inherit from the nn.Module and implement the two methods __init__ and forward\n",
        "class MLP5BN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(1024, 64),\n",
        "            nn.BatchNorm1d(64), # batch normalization layer with the number of features equal to 64\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, output_len)\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # The forward method defines the forward pass through the network.\n",
        "        x = self.flatten(x)\n",
        "        return self.linear_relu_stack(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelMLP5BN = MLP5BN().to(device)\n",
        "\n",
        "#Using stochastic gradient descent for the optimizer and 1e-3 for step-size\n",
        "learning_rate = 1e-3\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(modelMLP5BN.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "Y7ptt2wDsutC"
      },
      "id": "Y7ptt2wDsutC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, modelMLP5BN, loss_fn, optimizer, device)\n",
        "    evaluate_loop(val_dataloader, modelMLP5BN, loss_fn, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPA0O3mYsRFO",
        "outputId": "5473d3eb-9d9b-440a-8966-139c39e3c925"
      },
      "id": "xPA0O3mYsRFO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.389149  [   64 / 40000]\n",
            "loss: 2.340690  [ 6464 / 40000]\n",
            "loss: 2.224452  [12864 / 40000]\n",
            "loss: 2.278910  [19264 / 40000]\n",
            "loss: 2.107138  [25664 / 40000]\n",
            "loss: 2.205730  [32064 / 40000]\n",
            "loss: 2.135494  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 24.2%, Avg loss: 2.148784 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.174519  [   64 / 40000]\n",
            "loss: 2.074400  [ 6464 / 40000]\n",
            "loss: 2.148257  [12864 / 40000]\n",
            "loss: 2.089277  [19264 / 40000]\n",
            "loss: 2.063035  [25664 / 40000]\n",
            "loss: 2.016971  [32064 / 40000]\n",
            "loss: 2.074417  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 28.5%, Avg loss: 2.069112 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.048801  [   64 / 40000]\n",
            "loss: 2.048280  [ 6464 / 40000]\n",
            "loss: 2.125125  [12864 / 40000]\n",
            "loss: 1.941204  [19264 / 40000]\n",
            "loss: 2.034583  [25664 / 40000]\n",
            "loss: 1.899674  [32064 / 40000]\n",
            "loss: 2.010813  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 30.3%, Avg loss: 2.012336 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.927053  [   64 / 40000]\n",
            "loss: 1.968398  [ 6464 / 40000]\n",
            "loss: 1.879033  [12864 / 40000]\n",
            "loss: 2.033767  [19264 / 40000]\n",
            "loss: 2.041397  [25664 / 40000]\n",
            "loss: 2.070972  [32064 / 40000]\n",
            "loss: 2.182067  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 31.5%, Avg loss: 1.973521 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.901388  [   64 / 40000]\n",
            "loss: 1.998157  [ 6464 / 40000]\n",
            "loss: 1.955956  [12864 / 40000]\n",
            "loss: 2.041817  [19264 / 40000]\n",
            "loss: 2.036254  [25664 / 40000]\n",
            "loss: 1.720875  [32064 / 40000]\n",
            "loss: 1.953830  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 32.0%, Avg loss: 1.940245 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.931472  [   64 / 40000]\n",
            "loss: 1.942632  [ 6464 / 40000]\n",
            "loss: 1.886744  [12864 / 40000]\n",
            "loss: 1.902552  [19264 / 40000]\n",
            "loss: 1.927062  [25664 / 40000]\n",
            "loss: 1.894354  [32064 / 40000]\n",
            "loss: 1.870807  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 32.6%, Avg loss: 1.916179 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.004875  [   64 / 40000]\n",
            "loss: 1.884143  [ 6464 / 40000]\n",
            "loss: 1.966750  [12864 / 40000]\n",
            "loss: 1.870394  [19264 / 40000]\n",
            "loss: 2.022009  [25664 / 40000]\n",
            "loss: 2.040984  [32064 / 40000]\n",
            "loss: 1.957458  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 33.0%, Avg loss: 1.898133 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.877117  [   64 / 40000]\n",
            "loss: 1.958511  [ 6464 / 40000]\n",
            "loss: 1.812323  [12864 / 40000]\n",
            "loss: 1.895051  [19264 / 40000]\n",
            "loss: 1.945972  [25664 / 40000]\n",
            "loss: 1.949977  [32064 / 40000]\n",
            "loss: 1.772037  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 33.9%, Avg loss: 1.876752 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.917634  [   64 / 40000]\n",
            "loss: 1.789766  [ 6464 / 40000]\n",
            "loss: 1.883524  [12864 / 40000]\n",
            "loss: 1.692684  [19264 / 40000]\n",
            "loss: 1.812037  [25664 / 40000]\n",
            "loss: 1.747778  [32064 / 40000]\n",
            "loss: 1.783937  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 34.3%, Avg loss: 1.862529 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.872522  [   64 / 40000]\n",
            "loss: 1.761479  [ 6464 / 40000]\n",
            "loss: 1.847592  [12864 / 40000]\n",
            "loss: 1.873370  [19264 / 40000]\n",
            "loss: 1.967126  [25664 / 40000]\n",
            "loss: 2.007814  [32064 / 40000]\n",
            "loss: 1.733587  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 35.1%, Avg loss: 1.844729 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When using the Gradient descent as optimizer we do not notice a large difference between the performance of MLP5BN and the one of MLP5. However we can use here another optimizer : the adaptative moment estimation."
      ],
      "metadata": {
        "id": "g-D1nd7OUYua"
      },
      "id": "g-D1nd7OUYua"
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=modelMLP5BN.parameters(), lr=1e-3) #adaptative moment optimizer"
      ],
      "metadata": {
        "id": "mOAmEVdOTTmV"
      },
      "id": "mOAmEVdOTTmV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, modelMLP5BN, loss_fn, optimizer, device)\n",
        "    evaluate_loop(val_dataloader, modelMLP5BN, loss_fn, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zlGeBD8TZSz",
        "outputId": "b1b18930-488f-4acf-db9b-3dc0147fddd6"
      },
      "id": "3zlGeBD8TZSz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.860796  [   64 / 40000]\n",
            "loss: 1.807316  [ 6464 / 40000]\n",
            "loss: 1.904148  [12864 / 40000]\n",
            "loss: 1.735212  [19264 / 40000]\n",
            "loss: 1.742115  [25664 / 40000]\n",
            "loss: 1.741662  [32064 / 40000]\n",
            "loss: 1.844354  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 35.4%, Avg loss: 1.798849 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.832559  [   64 / 40000]\n",
            "loss: 1.751490  [ 6464 / 40000]\n",
            "loss: 1.788820  [12864 / 40000]\n",
            "loss: 1.766095  [19264 / 40000]\n",
            "loss: 1.633330  [25664 / 40000]\n",
            "loss: 1.533624  [32064 / 40000]\n",
            "loss: 1.549862  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 37.5%, Avg loss: 1.751376 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.494072  [   64 / 40000]\n",
            "loss: 1.614352  [ 6464 / 40000]\n",
            "loss: 1.590360  [12864 / 40000]\n",
            "loss: 1.584679  [19264 / 40000]\n",
            "loss: 1.711880  [25664 / 40000]\n",
            "loss: 1.637505  [32064 / 40000]\n",
            "loss: 1.713174  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 38.9%, Avg loss: 1.723345 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.572626  [   64 / 40000]\n",
            "loss: 1.673965  [ 6464 / 40000]\n",
            "loss: 1.324533  [12864 / 40000]\n",
            "loss: 1.513241  [19264 / 40000]\n",
            "loss: 1.495979  [25664 / 40000]\n",
            "loss: 1.483034  [32064 / 40000]\n",
            "loss: 1.562798  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 41.3%, Avg loss: 1.652195 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.567138  [   64 / 40000]\n",
            "loss: 1.483893  [ 6464 / 40000]\n",
            "loss: 1.702767  [12864 / 40000]\n",
            "loss: 1.713346  [19264 / 40000]\n",
            "loss: 1.690874  [25664 / 40000]\n",
            "loss: 1.808479  [32064 / 40000]\n",
            "loss: 1.426063  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 41.0%, Avg loss: 1.663104 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.465108  [   64 / 40000]\n",
            "loss: 1.314746  [ 6464 / 40000]\n",
            "loss: 1.458716  [12864 / 40000]\n",
            "loss: 1.486843  [19264 / 40000]\n",
            "loss: 1.646921  [25664 / 40000]\n",
            "loss: 1.678788  [32064 / 40000]\n",
            "loss: 1.599355  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 40.6%, Avg loss: 1.672227 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.465634  [   64 / 40000]\n",
            "loss: 1.433589  [ 6464 / 40000]\n",
            "loss: 1.683228  [12864 / 40000]\n",
            "loss: 1.528462  [19264 / 40000]\n",
            "loss: 1.489538  [25664 / 40000]\n",
            "loss: 1.691583  [32064 / 40000]\n",
            "loss: 1.526009  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 39.2%, Avg loss: 1.709684 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.294637  [   64 / 40000]\n",
            "loss: 1.449682  [ 6464 / 40000]\n",
            "loss: 1.544939  [12864 / 40000]\n",
            "loss: 1.242780  [19264 / 40000]\n",
            "loss: 1.854983  [25664 / 40000]\n",
            "loss: 1.399308  [32064 / 40000]\n",
            "loss: 1.662730  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 41.5%, Avg loss: 1.664706 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.235197  [   64 / 40000]\n",
            "loss: 1.282708  [ 6464 / 40000]\n",
            "loss: 1.379977  [12864 / 40000]\n",
            "loss: 1.339681  [19264 / 40000]\n",
            "loss: 1.610676  [25664 / 40000]\n",
            "loss: 1.430252  [32064 / 40000]\n",
            "loss: 1.289753  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 41.4%, Avg loss: 1.672649 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.456314  [   64 / 40000]\n",
            "loss: 1.449995  [ 6464 / 40000]\n",
            "loss: 1.271609  [12864 / 40000]\n",
            "loss: 1.450765  [19264 / 40000]\n",
            "loss: 1.641557  [25664 / 40000]\n",
            "loss: 1.381797  [32064 / 40000]\n",
            "loss: 1.578454  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 40.1%, Avg loss: 1.719996 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the MLP5BN is much better since the accuracies and the average losses are better. However, it seems that, with this optimizer, the accuracy is not increasing from one epoch to the next anymore.\n"
      ],
      "metadata": {
        "id": "Leo_wnD7V2Lg"
      },
      "id": "Leo_wnD7V2Lg"
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating on the test dataloader\n",
        "print(\"model MLP5\")\n",
        "evaluate_loop(test_dataloader, modelMLP5, loss_fn, device, set_= 'Test')\n",
        "print(\"model MLP5BN\")\n",
        "evaluate_loop(test_dataloader, modelMLP5BN, loss_fn, device, set_= 'Test' )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3qBZUg8dLrP",
        "outputId": "7cabd0c0-cae9-4aa0-f3e6-a6717a13fe9e"
      },
      "id": "J3qBZUg8dLrP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model MLP5\n",
            "\n",
            "Test set: \n",
            "  Accuracy: 34.0%, Avg loss: 1.837977 \n",
            "\n",
            "model MLP5BN\n",
            "\n",
            "Test set: \n",
            "  Accuracy: 40.4%, Avg loss: 1.717094 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the model MLP5BN is better than the MLP5"
      ],
      "metadata": {
        "id": "IU2xOWpKfv13"
      },
      "id": "IU2xOWpKfv13"
    },
    {
      "cell_type": "markdown",
      "id": "df4b9bf7-aef8-41d2-badd-98f4f55f6f9c",
      "metadata": {
        "id": "df4b9bf7-aef8-41d2-badd-98f4f55f6f9c"
      },
      "source": [
        "## Exercise 9\n",
        "The following Python class `LeNet5` is an implementation of a Convolutional Neural Network (CNN).\n",
        "Train this model and compare it with the previous two models. (Ignore the `num_channels` parameter of the class for now. Set it to the default value `1`.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd362515-d777-450b-91f2-9bdb8933dfaf",
      "metadata": {
        "scrolled": true,
        "id": "fd362515-d777-450b-91f2-9bdb8933dfaf"
      },
      "outputs": [],
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(6),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(400, 120)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(120, 84)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(84, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)  # Second dimension is for channels, but we only have one channel.\n",
        "        out = self.layer2(out)\n",
        "        out = self.flatten(out)\n",
        "        out = self.fc(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution of exercise 9"
      ],
      "metadata": {
        "id": "izpIBx4Xr1wN"
      },
      "id": "izpIBx4Xr1wN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "522d6194-11e9-43a1-b350-348dd30c1f59",
      "metadata": {
        "scrolled": true,
        "id": "522d6194-11e9-43a1-b350-348dd30c1f59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1276d6fd-aba2-43c1-b530-7795433ef057"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.304760  [   64 / 40000]\n",
            "loss: 1.720094  [ 6464 / 40000]\n",
            "loss: 1.678812  [12864 / 40000]\n",
            "loss: 1.773762  [19264 / 40000]\n",
            "loss: 1.307057  [25664 / 40000]\n",
            "loss: 1.773493  [32064 / 40000]\n",
            "loss: 1.312226  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 48.3%, Avg loss: 1.437194 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.456172  [   64 / 40000]\n",
            "loss: 1.553846  [ 6464 / 40000]\n",
            "loss: 1.510741  [12864 / 40000]\n",
            "loss: 1.249489  [19264 / 40000]\n",
            "loss: 1.200642  [25664 / 40000]\n",
            "loss: 1.131436  [32064 / 40000]\n",
            "loss: 1.233128  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 52.2%, Avg loss: 1.339954 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.345139  [   64 / 40000]\n",
            "loss: 1.236427  [ 6464 / 40000]\n",
            "loss: 1.239468  [12864 / 40000]\n",
            "loss: 1.176434  [19264 / 40000]\n",
            "loss: 1.114051  [25664 / 40000]\n",
            "loss: 1.037642  [32064 / 40000]\n",
            "loss: 1.141836  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 52.7%, Avg loss: 1.386907 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.058490  [   64 / 40000]\n",
            "loss: 1.190001  [ 6464 / 40000]\n",
            "loss: 1.138192  [12864 / 40000]\n",
            "loss: 1.242778  [19264 / 40000]\n",
            "loss: 1.013639  [25664 / 40000]\n",
            "loss: 1.155728  [32064 / 40000]\n",
            "loss: 1.133197  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 50.8%, Avg loss: 1.450513 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.992195  [   64 / 40000]\n",
            "loss: 0.662150  [ 6464 / 40000]\n",
            "loss: 1.020645  [12864 / 40000]\n",
            "loss: 0.903340  [19264 / 40000]\n",
            "loss: 0.843788  [25664 / 40000]\n",
            "loss: 0.936117  [32064 / 40000]\n",
            "loss: 1.134481  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 60.5%, Avg loss: 1.139000 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.341680  [   64 / 40000]\n",
            "loss: 1.036327  [ 6464 / 40000]\n",
            "loss: 1.008595  [12864 / 40000]\n",
            "loss: 1.026354  [19264 / 40000]\n",
            "loss: 1.016091  [25664 / 40000]\n",
            "loss: 1.243921  [32064 / 40000]\n",
            "loss: 0.909669  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 58.4%, Avg loss: 1.228696 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.909599  [   64 / 40000]\n",
            "loss: 1.025628  [ 6464 / 40000]\n",
            "loss: 1.110672  [12864 / 40000]\n",
            "loss: 1.017119  [19264 / 40000]\n",
            "loss: 0.824185  [25664 / 40000]\n",
            "loss: 1.248397  [32064 / 40000]\n",
            "loss: 0.941956  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 58.1%, Avg loss: 1.219317 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.845580  [   64 / 40000]\n",
            "loss: 0.787024  [ 6464 / 40000]\n",
            "loss: 1.244962  [12864 / 40000]\n",
            "loss: 1.071909  [19264 / 40000]\n",
            "loss: 1.075443  [25664 / 40000]\n",
            "loss: 0.948078  [32064 / 40000]\n",
            "loss: 1.062313  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 51.8%, Avg loss: 1.512036 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.911385  [   64 / 40000]\n",
            "loss: 1.032109  [ 6464 / 40000]\n",
            "loss: 0.735405  [12864 / 40000]\n",
            "loss: 1.174812  [19264 / 40000]\n",
            "loss: 0.692787  [25664 / 40000]\n",
            "loss: 0.909551  [32064 / 40000]\n",
            "loss: 1.055401  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 60.0%, Avg loss: 1.171039 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.753512  [   64 / 40000]\n",
            "loss: 0.775792  [ 6464 / 40000]\n",
            "loss: 0.843833  [12864 / 40000]\n",
            "loss: 0.715454  [19264 / 40000]\n",
            "loss: 0.880880  [25664 / 40000]\n",
            "loss: 0.769119  [32064 / 40000]\n",
            "loss: 0.825130  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 58.4%, Avg loss: 1.227019 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "modelLeNet5 = LeNet5().to(device) # creating an instance of the classe and moving it to the device\n",
        "loss_fn=nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=modelLeNet5.parameters(), lr=1e-3) #adaptative moment optimizer\n",
        "epochs =10\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, modelLeNet5, loss_fn, optimizer, device)\n",
        "    evaluate_loop(val_dataloader, modelLeNet5, loss_fn, device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating on the test dataloader\n",
        "print(\"model MLP5\")\n",
        "evaluate_loop(test_dataloader, modelMLP5, loss_fn, device, set_= 'Test')\n",
        "print(\"model MLP5BN\")\n",
        "evaluate_loop(test_dataloader, modelMLP5BN, loss_fn, device, set_= 'Test' )\n",
        "print(\"model LeNet5\")\n",
        "evaluate_loop(test_dataloader, modelLeNet5, loss_fn, device, set_= 'Test' )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMcszfCrgMld",
        "outputId": "65597fda-b5c4-4952-c962-2e146c99c73f"
      },
      "id": "KMcszfCrgMld",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model MLP5\n",
            "\n",
            "Test set: \n",
            "  Accuracy: 34.0%, Avg loss: 1.840185 \n",
            "\n",
            "model MLP5BN\n",
            "\n",
            "Test set: \n",
            "  Accuracy: 40.4%, Avg loss: 1.717322 \n",
            "\n",
            "model LeNet5\n",
            "\n",
            "Test set: \n",
            "  Accuracy: 57.7%, Avg loss: 1.241938 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the LeNet model performs better than the previous two models since the accuracy and the average loss are better"
      ],
      "metadata": {
        "id": "BLwT403MY_v3"
      },
      "id": "BLwT403MY_v3"
    },
    {
      "cell_type": "markdown",
      "id": "87b5f75c-79c9-4058-995c-c73d176ab7f8",
      "metadata": {
        "id": "87b5f75c-79c9-4058-995c-c73d176ab7f8"
      },
      "source": [
        "## Exercise 10\n",
        "Below is the same dataset but with colors. For expressing the intensities for the red, green, blue colors, each image has 3 *channels* now, which is why the shape of each image is `(3, 32, 32)`.\n",
        "The goal of this exercise is to train a classifier using `LeNet5`, but the `LeNet5` class defined above assumes that input images have only one channel, so we need to rewrite the class a bit. For that, read [the documentation for `torch.nn.Conv2d` class](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=conv2d#torch.nn.Conv2d) to understand how to modify the first layer of the `LeNet5` class. Write code for this modified class, naming it `LeNet5Color`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71796fb0-1f80-4a5c-9753-4648c48ad3ad",
      "metadata": {
        "id": "71796fb0-1f80-4a5c-9753-4648c48ad3ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "fda457ac-78b0-43bd-b7b1-8da8af63bdc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['truck', 'truck', 'deer', 'automobile', 'automobile']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x200 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of an image: (3, 32, 32)\n"
          ]
        }
      ],
      "source": [
        "x_train_val_np_color, y_train_val_np_color = load_cifar10(train=True, color=True)\n",
        "x_test_np_color, y_test_np_color = load_cifar10(train=False, color=True)\n",
        "plot_images(x_train_val_np_color, y_train_val_np_color, rows=1, cols=5, color=True)\n",
        "print(f'Shape of an image: {x_train_val_np_color[0].shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution of exercise 10"
      ],
      "metadata": {
        "id": "uwx9veIn4c4b"
      },
      "id": "uwx9veIn4c4b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c1aa3d8-0d79-4747-828c-7ac91393712e",
      "metadata": {
        "id": "2c1aa3d8-0d79-4747-828c-7ac91393712e"
      },
      "outputs": [],
      "source": [
        "class LeNet5Color(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(LeNet5Color, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 6, kernel_size=5, stride=1, padding=0), #we modify the first parameter(in_channels) from 1 to 3 because each image has 3 channels now\n",
        "            nn.BatchNorm2d(6),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(400, 120)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(120, 84)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(84, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.flatten(out)\n",
        "        out = self.fc(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cd0acd0-b720-41e2-a6e4-0e7ac13070a1",
      "metadata": {
        "id": "6cd0acd0-b720-41e2-a6e4-0e7ac13070a1"
      },
      "source": [
        "## Exercise 11\n",
        "Split the data, create dataloaders, train an instance of `LeNet5Color`, and compare it with `LeNet5` using the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution of exercise 11"
      ],
      "metadata": {
        "id": "C-o2JzQxEsv-"
      },
      "id": "C-o2JzQxEsv-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70673a87-abb2-4414-9c08-7aaf1c4a49cf",
      "metadata": {
        "id": "70673a87-abb2-4414-9c08-7aaf1c4a49cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37c146e8-1915-49fb-add8-9f08316b99da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-d19eb33125b3>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.X = torch.tensor(X, dtype=torch.float32) # applying the transform ToTensor() and changing the dtype directly\n",
            "<ipython-input-12-d19eb33125b3>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.y = torch.tensor(y, dtype=torch.int64) # applying the transform ToTensor() and changing the dtype directly\n"
          ]
        }
      ],
      "source": [
        "#Split data\n",
        "x_train_np_col, x_val_np_col, y_train_np_col, y_val_np_col = train_test_split(x_train_val_np_color, y_train_val_np_color, train_size=40000,\n",
        "                                                              stratify = y_train_val_np_color, random_state = 6)\n",
        "\n",
        "# create tensors\n",
        "x_train_col = torch.from_numpy(x_train_np_col).to(dtype=torch.float32)\n",
        "y_train_col = torch.from_numpy(y_train_np_col).to(dtype=torch.int64)\n",
        "x_val_col = torch.from_numpy(x_val_np_col).to(dtype=torch.float32)\n",
        "y_val_col = torch.from_numpy(y_val_np_col).to(dtype=torch.int64)\n",
        "x_test_col = torch.from_numpy(x_test_np_color).to(dtype=torch.float32)\n",
        "y_test_col = torch.from_numpy(y_test_np_color).to(dtype=torch.int64)\n",
        "\n",
        "#create dataloaders\n",
        "train_dataloader_color = DataLoader(CustomDataset(x_train_col, y_train_col), batch_size=64, shuffle=True)\n",
        "val_dataloader_color = DataLoader(CustomDataset(x_val_col, y_val_col), batch_size=64, shuffle=True)\n",
        "test_dataloader_color = DataLoader(CustomDataset(x_test_col, y_test_col), batch_size=64, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelLeNet5Color = LeNet5Color().to(device) # an instance of LeNet5Color moved to device\n",
        "loss_fn=nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=modelLeNet5Color.parameters(), lr=1e-3) #adaptative moment optimizer\n",
        "\n",
        "#train the model\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader_color, modelLeNet5Color, loss_fn, optimizer, device)\n",
        "    evaluate_loop(val_dataloader_color, modelLeNet5Color, loss_fn, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOlA8nbfEmXE",
        "outputId": "6956bd22-c76e-405a-ae8a-6b8119f2ae46"
      },
      "id": "yOlA8nbfEmXE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.321281  [   64 / 40000]\n",
            "loss: 1.544191  [ 6464 / 40000]\n",
            "loss: 1.296797  [12864 / 40000]\n",
            "loss: 1.376937  [19264 / 40000]\n",
            "loss: 1.503104  [25664 / 40000]\n",
            "loss: 1.311227  [32064 / 40000]\n",
            "loss: 1.429634  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 46.1%, Avg loss: 1.493958 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.270095  [   64 / 40000]\n",
            "loss: 1.327270  [ 6464 / 40000]\n",
            "loss: 1.208655  [12864 / 40000]\n",
            "loss: 1.144135  [19264 / 40000]\n",
            "loss: 1.316510  [25664 / 40000]\n",
            "loss: 1.229648  [32064 / 40000]\n",
            "loss: 1.132710  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 53.8%, Avg loss: 1.298975 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.086879  [   64 / 40000]\n",
            "loss: 0.877719  [ 6464 / 40000]\n",
            "loss: 1.180376  [12864 / 40000]\n",
            "loss: 1.239689  [19264 / 40000]\n",
            "loss: 1.179618  [25664 / 40000]\n",
            "loss: 1.146882  [32064 / 40000]\n",
            "loss: 1.242786  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 56.8%, Avg loss: 1.237095 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.089085  [   64 / 40000]\n",
            "loss: 1.094356  [ 6464 / 40000]\n",
            "loss: 1.106365  [12864 / 40000]\n",
            "loss: 0.964995  [19264 / 40000]\n",
            "loss: 1.026598  [25664 / 40000]\n",
            "loss: 1.074049  [32064 / 40000]\n",
            "loss: 1.024468  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 56.2%, Avg loss: 1.244109 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.933936  [   64 / 40000]\n",
            "loss: 0.913642  [ 6464 / 40000]\n",
            "loss: 1.015743  [12864 / 40000]\n",
            "loss: 1.084138  [19264 / 40000]\n",
            "loss: 0.880095  [25664 / 40000]\n",
            "loss: 1.081092  [32064 / 40000]\n",
            "loss: 0.906129  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 60.3%, Avg loss: 1.133188 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.904146  [   64 / 40000]\n",
            "loss: 0.707465  [ 6464 / 40000]\n",
            "loss: 0.728375  [12864 / 40000]\n",
            "loss: 1.038423  [19264 / 40000]\n",
            "loss: 1.206799  [25664 / 40000]\n",
            "loss: 1.088953  [32064 / 40000]\n",
            "loss: 1.078311  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 60.9%, Avg loss: 1.134867 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.090377  [   64 / 40000]\n",
            "loss: 0.775121  [ 6464 / 40000]\n",
            "loss: 0.944519  [12864 / 40000]\n",
            "loss: 1.225853  [19264 / 40000]\n",
            "loss: 0.925776  [25664 / 40000]\n",
            "loss: 0.886493  [32064 / 40000]\n",
            "loss: 0.947184  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 58.2%, Avg loss: 1.207579 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.865678  [   64 / 40000]\n",
            "loss: 0.815734  [ 6464 / 40000]\n",
            "loss: 0.814611  [12864 / 40000]\n",
            "loss: 0.908237  [19264 / 40000]\n",
            "loss: 1.061519  [25664 / 40000]\n",
            "loss: 0.780533  [32064 / 40000]\n",
            "loss: 1.012808  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 57.5%, Avg loss: 1.292747 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.973550  [   64 / 40000]\n",
            "loss: 0.808322  [ 6464 / 40000]\n",
            "loss: 0.753405  [12864 / 40000]\n",
            "loss: 0.902016  [19264 / 40000]\n",
            "loss: 0.760844  [25664 / 40000]\n",
            "loss: 0.918252  [32064 / 40000]\n",
            "loss: 0.779993  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 59.0%, Avg loss: 1.264612 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.718475  [   64 / 40000]\n",
            "loss: 0.859372  [ 6464 / 40000]\n",
            "loss: 0.738936  [12864 / 40000]\n",
            "loss: 0.944914  [19264 / 40000]\n",
            "loss: 0.802655  [25664 / 40000]\n",
            "loss: 0.899347  [32064 / 40000]\n",
            "loss: 0.795177  [38464 / 40000]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 62.8%, Avg loss: 1.076955 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating on the test dataloader without colors\n",
        "\n",
        "print(\"model LeNet5\")\n",
        "evaluate_loop(test_dataloader, modelLeNet5, loss_fn, device, set_= 'Test' )\n",
        "\n",
        "print(\"model LeNet5Color\")\n",
        "evaluate_loop(test_dataloader, modelLeNet5Color, loss_fn, device, set_= 'Test' )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "aPAbi-cYhYP1",
        "outputId": "21db3a47-06d5-435f-ca3c-ac8c96ab6c81"
      },
      "id": "aPAbi-cYhYP1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model LeNet5\n",
            "\n",
            "Test set: \n",
            "  Accuracy: 57.7%, Avg loss: 1.245549 \n",
            "\n",
            "model LeNet5Color\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-e69ef9f4d499>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model LeNet5Color\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mevaluate_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelLeNet5Color\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'Test'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-4d7eb7a8b655>\u001b[0m in \u001b[0;36mevaluate_loop\u001b[0;34m(dataloader, model, loss_fn, device, set_)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-7297514dc94c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [6, 3, 5, 5], expected input[64, 1, 32, 32] to have 3 channels, but got 1 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating on the test dataloader with colors\n",
        "\n",
        "print(\"model LeNet5\")\n",
        "evaluate_loop(test_dataloader_color, modelLeNet5, loss_fn, device, set_= 'Test' )\n",
        "\n",
        "print(\"model LeNet5Color\")\n",
        "evaluate_loop(test_dataloader_color, modelLeNet5Color, loss_fn, device, set_= 'Test' )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "O1jPemmajxkb",
        "outputId": "4bbf4b40-6cd4-498a-b693-034fe2064877"
      },
      "id": "O1jPemmajxkb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model LeNet5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-e5581110aa65>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model LeNet5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mevaluate_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader_color\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelLeNet5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'Test'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model LeNet5Color\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-4d7eb7a8b655>\u001b[0m in \u001b[0;36mevaluate_loop\u001b[0;34m(dataloader, model, loss_fn, device, set_)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-3bd34b16b851>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Second dimension is for channels, but we only have one channel.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [6, 1, 5, 5], expected input[64, 3, 32, 32] to have 1 channels, but got 3 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get an error if we use the test dataloader (without colors) for testing the two models. This is because the model LeNet5Color is expected data (images) with colors. Likewise, we also get an error if we use the test dataloader (with colors) for testing the two models since LeNet5 is expected data(images) without colors. So the two models are not comparable on the same basis.\n",
        "\n",
        "We can show the test accuracy on each suitable test dataloader"
      ],
      "metadata": {
        "id": "LyGpQQDcjAff"
      },
      "id": "LyGpQQDcjAff"
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating on the test dataloader, one with colors and the second one without\n",
        "\n",
        "print(\"model LeNet5\")\n",
        "evaluate_loop(test_dataloader, modelLeNet5, loss_fn, device, set_= 'Test' )\n",
        "\n",
        "print(\"model LeNet5Color\")\n",
        "evaluate_loop(test_dataloader_color, modelLeNet5Color, loss_fn, device, set_= 'Test' )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3PjP_8Qi_49",
        "outputId": "0b3e8741-dc90-471d-8f0d-23a51f4cceb8"
      },
      "id": "H3PjP_8Qi_49",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model LeNet5\n",
            "\n",
            "Test set: \n",
            "  Accuracy: 57.7%, Avg loss: 1.244318 \n",
            "\n",
            "model LeNet5Color\n",
            "\n",
            "Test set: \n",
            "  Accuracy: 63.5%, Avg loss: 1.071223 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74fa2d43-fea2-4615-940f-a831e1438bfc",
      "metadata": {
        "scrolled": true,
        "id": "74fa2d43-fea2-4615-940f-a831e1438bfc"
      },
      "source": [
        "## Exercise 12\n",
        "- Randomly take 10 examples from each class of CIFAR10 to make a subset (*) of the dataset.\n",
        "- Load this model ResNet18 pre-trained on ImageNet v1 which is available from `torchvision.models`. See [this](https://pytorch.org/vision/stable/models.html) and [this documentation](https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet18.html#torchvision.models.resnet18).\n",
        "- Fine-tune the pre-trained model for 10-class classification with your subset (from (*)) of CIFAR10 (with colors). Train the model for 5 epochs.\n",
        "- You may need to carefully read [this documentation](https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet18.html#torchvision.models.resnet18) to do the right data pre-processing.\n",
        "\n",
        "For this exercise, what is important is to have correct code. The final accuracy is less important, so you don't need to spend too much time on tuning hyper-parameters. There is a big difference in the image sizes, and it is challenging to make this transfer learning successful."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution of exercise 12"
      ],
      "metadata": {
        "id": "1s8_TLYD8ZSJ"
      },
      "id": "1s8_TLYD8ZSJ"
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_col.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBX0jUfT8Y7O",
        "outputId": "ed6a8c9f-155d-4145-d787-7ff55ff61835"
      },
      "id": "sBX0jUfT8Y7O",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([40000, 3, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes =  np.unique(y_train_col) #the different labels\n",
        "\n",
        "X_col = [[x_train_col[i,:,:,:] for i in range(len(x_train_col)) if y_train_col[i]==classe] for classe in classes] #data regrouped by labels in different lists\n",
        "y_col = [[y_train_col[i] for i in range(len(y_train_col)) if y_train_col[i]==classe] for classe in classes] #target variable repgrouped by labels in different lists\n",
        "x_subset_cifar = []\n",
        "y_subset_cifar = []\n",
        "\n",
        "#Randomly take 10 examples from each class\n",
        "for classe in classes :\n",
        "  sublist = np.random.random_integers(0, len(X_col[classe])-1, 10) # random list of indexes per label, a list of len 10\n",
        "  x_subset_cifar += [X_col[classe][i] for i in sublist] # 10 randomly took examples for this class\n",
        "  y_subset_cifar += [y_col[classe][i] for i in sublist] # the corresponding labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlBFisxY1g84",
        "outputId": "8578a48f-c144-4dcb-8bf2-1e242a50b7a5"
      },
      "id": "jlBFisxY1g84",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-39-40aff5621c23>:10: DeprecationWarning: This function is deprecated. Please call randint(0, 3999 + 1) instead\n",
            "  sublist = np.random.random_integers(0, len(X_col[classe])-1, 10) # random list of indexes per label, a list of len 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_subset_cifar = torch.from_numpy(np.concatenate([t[np.newaxis, ...] for t in x_subset_cifar], axis=0)).to(dtype = torch.float32) # creating the tensor from the list\n",
        "y_subset_cifar = torch.from_numpy(np.array(y_subset_cifar)).to(dtype = torch.int64) #creating the tensor from the list\n",
        "print(x_subset_cifar.shape)\n",
        "print(y_subset_cifar.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oBUzfyI64sv",
        "outputId": "f26b1ab3-5a0f-4799-c1d1-2a0c6186da81"
      },
      "id": "5oBUzfyI64sv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 3, 32, 32])\n",
            "torch.Size([100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "Resnet18 = resnet18(pretrained=True) #loading pre-trained resnet18"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdPMQV3R9z2o",
        "outputId": "76a6a59e-3a7b-4939-96d2-0e4b777f2e49"
      },
      "id": "QdPMQV3R9z2o",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|| 44.7M/44.7M [00:00<00:00, 151MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Fine tune the pre-trained model\n",
        "Resnet18.fc = nn.Linear(Resnet18.fc.in_features, 10)  # 10 output classes for CIFAR-10\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=Resnet18.parameters(), lr=1e-3)\n",
        "\n",
        "# Train the model\n",
        "epochs = 5\n",
        "Resnet18.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGD2HUc3_AE6",
        "outputId": "f15777fb-2a78-46e9-c42b-745ce5017419"
      },
      "id": "uGD2HUc3_AE6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transf = ResNet18_Weights.IMAGENET1K_V1.transforms() #transformations from the documentation of Resnet18\n",
        "train_dataloader_resnet = DataLoader(CustomDataset(x_subset_cifar, y_subset_cifar, transform = transf), batch_size=64, shuffle=True) #create the dataloader with the suitable preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7wgwei6AJVj",
        "outputId": "34d10ee4-6101-41e5-81af-b5ac8c05b633"
      },
      "id": "q7wgwei6AJVj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-d19eb33125b3>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.X = torch.tensor(X, dtype=torch.float32) # applying the transform ToTensor() and changing the dtype directly\n",
            "<ipython-input-12-d19eb33125b3>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.y = torch.tensor(y, dtype=torch.int64) # applying the transform ToTensor() and changing the dtype directly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model with the subset and with 5 epochs\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader_resnet, Resnet18, loss_fn, optimizer, device) # training with the subset\n",
        "    evaluate_loop(val_dataloader_color, Resnet18, loss_fn, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xawc0mOB__ys",
        "outputId": "93c11932-935d-479c-8b2d-0981ee5d8eab"
      },
      "id": "Xawc0mOB__ys",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.425825  [   64 /   100]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 15.9%, Avg loss: 2.338558 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.340722  [   64 /   100]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 13.4%, Avg loss: 2.391738 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.051154  [   64 /   100]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 10.7%, Avg loss: 2.439552 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.076581  [   64 /   100]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 10.1%, Avg loss: 2.459397 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.015164  [   64 /   100]\n",
            "\n",
            "Validation set: \n",
            "  Accuracy: 9.9%, Avg loss: 2.466003 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate on the test dataloader\n",
        "print(\"model Resnet18\")\n",
        "evaluate_loop(test_dataloader_color, Resnet18, loss_fn, device, set_ = 'Test')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aCdHe39pcIS",
        "outputId": "9f06af18-4255-48b5-d5d8-de0435a9e611"
      },
      "id": "4aCdHe39pcIS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model Resnet18\n",
            "\n",
            "Test set: \n",
            "  Accuracy: 11.3%, Avg loss: 2.464847 \n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}